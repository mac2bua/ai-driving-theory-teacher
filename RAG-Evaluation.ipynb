{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb65a95d-3639-409b-b84c-a16bbe7d3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad1a94a-241e-4152-b6c7-dd0e43794839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load questions\n",
    "with open('data/index_to_question.json', 'r') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index('data/questions_index.faiss')\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb7e68-55ae-49e7-b5fb-33b4a966d41c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e678fca4-952d-4801-a92c-822a5622958a",
   "metadata": {},
   "source": [
    "### Method 1: Simple prompt (GPT3.5 Turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2851489-5ca9-4632-ad73-1a81070f0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generated_answer(query, generated_answer, ground_truth_answer):\n",
    "    prompt = f\"\"\"\n",
    "You are to grade the following LLM answer on a scale from 0 to 10.\n",
    "\n",
    "**Question:**\n",
    "{query}\n",
    "\n",
    "**LLM Answer:**\n",
    "{generated_answer}\n",
    "\n",
    "**Correct Answer:**\n",
    "{ground_truth_answer}\n",
    "\n",
    "**Task:**\n",
    "- Assign a score from 0 (completely incorrect) to 10 (completely correct).\n",
    "- Provide a brief justification for the score.\n",
    "\n",
    "**Response Format:**\n",
    "Provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Score\": [0-10],\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8061198-1bf9-438a-863a-8ea48427c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_question(query, index, questions, embedding_model, k=5):\n",
    "    # Encode the query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "\n",
    "    # Search the index\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    # Retrieve the top k questions\n",
    "    retrieved_questions = [questions[idx] for idx in indices[0]]\n",
    "    return retrieved_questions, indices[0]\n",
    "\n",
    "def search_function(query):\n",
    "    return search_question(query, index, questions, embedding_model, k=5)\n",
    "\n",
    "with open('data/ground_truth.json', 'r') as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "\n",
    "def generate_answer(query, retrieved_docs):\n",
    "    context = \"\\n\".join([f\"Q: {doc['question']}\\nA: {doc['correct_options']}\" for doc in retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant helping users prepare for the German driving theory exam.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer the question using the context above, don't include the letter at the beginning of each correct option. If the answer is not in the context, say \"I'm sorry, I don't have enough information to answer that question.\"\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',  # Use 'gpt-4' if available\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "evaluations = []\n",
    "for item in ground_truth[:5]:\n",
    "    query = item['query']\n",
    "    ground_truth_answer = item['ground_truth_answer']\n",
    "    retrieved_docs, _ = search_function(query)\n",
    "    generated_answer = generate_answer(query, retrieved_docs)\n",
    "    evaluation = evaluate_generated_answer(query, generated_answer, ground_truth_answer)\n",
    "    evaluations.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de810c0-8cb1-4308-b3bf-e216a19873b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\\n  \"Score\": 10,\\n  \"Explanation\": \"The answer is completely correct and provides the accurate information that truck drivers in Germany are not permitted to have any alcohol in their system while operating a vehicle.\"\\n}',\n",
       " '{\\n  \"Score\": 2,\\n  \"Explanation\": \"The answer provided is incorrect as the legal limit for blood alcohol concentration for truck drivers in Germany is 0.3 grams per liter, not 0.02%. The answer shows a lack of accuracy and understanding of the topic.\"\\n}',\n",
       " '{\\n  \"Score\": 8,\\n  \"Explanation\": \"The answer correctly identifies the action to take when approaching a sharp curve while driving a truck, which is to reduce speed before entering the curve. However, it could have been more detailed by mentioning the importance of maintaining control of the vehicle and preventing rollovers or loss of control.\"\\n}',\n",
       " '{\\n  \"Score\": 8,\\n  \"Explanation\": \"The answer provided the correct legal limit for blood alcohol concentration (BAC) for truck drivers in Germany. However, it could have been improved by mentioning that the general limit for non-commercial drivers is 0.05% for a more comprehensive answer.\"\\n}',\n",
       " '{\\n  \"Score\": 9,\\n  \"Explanation\": \"The answer provided is almost completely correct, accurately stating the maximum permissible blood alcohol concentration (BAC) limit for truck drivers in Germany. The only improvement could be adding a bit more context or explanation about why this low limit is enforced.\"\\n}']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098c3c2e-ce9c-4f96-be2c-c8f10077bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = []\n",
    "for item in ground_truth:\n",
    "    query = item['query']\n",
    "    ground_truth_answer = item['ground_truth_answer']\n",
    "    retrieved_docs, _ = search_function(query)\n",
    "    generated_answer = generate_answer(query, retrieved_docs)\n",
    "    evaluation = evaluate_generated_answer(query, generated_answer, ground_truth_answer)\n",
    "    evaluations.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1906922-bb41-4a8e-b39b-2116bee95de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. score: 8.033333333333333\n"
     ]
    }
   ],
   "source": [
    "print(f\"Avg. score: {np.mean([json.loads(e)['Score'] for e in evaluations])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45384b71-b296-4943-a4f9-72e374da664f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c180044c-164b-4628-bf17-acda6156a1a5",
   "metadata": {},
   "source": [
    "### Method 2: Chain-of-Thought (GTP3.5 Turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d196cbf4-b970-41f8-8f19-b582f87cc38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_cot(query, retrieved_docs):\n",
    "    context = \"\\n\".join([f\"Q: {doc['question']}\\nA: {doc['correct_options']}\" for doc in retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant helping users prepare for the German driving theory exam.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question:\n",
    "    {query}\n",
    "    \n",
    "    Think step-by-step and provide a detailed answer based on the context. If the answer is not in the context, say \"I'm sorry, I don't have enough information to answer that question.\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbb53762-7c0c-4df3-805f-c59578300788",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_cot = []\n",
    "for item in ground_truth:\n",
    "    query = item['query']\n",
    "    ground_truth_answer = item['ground_truth_answer']\n",
    "    retrieved_docs, _ = search_function(query)\n",
    "    generated_answer = generate_answer_cot(query, retrieved_docs)\n",
    "    evaluation = evaluate_generated_answer(query, generated_answer, ground_truth_answer)\n",
    "    evaluations_cot.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b29a9b3b-3631-4448-a1fc-6379a964f793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. score: 8.033333333333333\n"
     ]
    }
   ],
   "source": [
    "print(f\"Avg. score: {np.mean([json.loads(e)['Score'] for e in evaluations_cot])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fa62fc-f18c-4fa4-8508-2f4e2e4f2413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "483b048d-e328-4aec-8907-5e415e51c4ae",
   "metadata": {},
   "source": [
    "### Method 3: Simple prompt (GPT-4o mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7911935a-1722-4e14-83e9-08d6bf8d7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_4o_mini(query, retrieved_docs):\n",
    "    context = \"\\n\".join([f\"Q: {doc['question']}\\nA: {doc['correct_options']}\" for doc in retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant helping users prepare for the German driving theory exam.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer the question using the context above, don't include the letter at the beginning of each correct option. If the answer is not in the context, say \"I'm sorry, I don't have enough information to answer that question.\"\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b7de539-3af1-4abb-9d29-fb50612c3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_4o = []\n",
    "for item in ground_truth:\n",
    "    query = item['query']\n",
    "    ground_truth_answer = item['ground_truth_answer']\n",
    "    retrieved_docs, _ = search_function(query)\n",
    "    generated_answer = generate_answer_4o_mini(query, retrieved_docs)\n",
    "    evaluation = evaluate_generated_answer(query, generated_answer, ground_truth_answer)\n",
    "    evaluations_4o.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ae73235-7624-442e-9691-c226fa6e5c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. score: 8.3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Avg. score: {np.mean([json.loads(e)['Score'] for e in evaluations_4o])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c4123-f486-45af-9c3e-0f077bae1290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4af5e797-7bf3-4852-a3cf-614603110142",
   "metadata": {},
   "source": [
    "### Method 4: Chain-of-Thought (GPT-4o mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8a11312-8f11-4612-8ca6-7ded25f882c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_cot_4o_mini(query, retrieved_docs):\n",
    "    context = \"\\n\".join([f\"Q: {doc['question']}\\nA: {doc['correct_options']}\" for doc in retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant helping users prepare for the German driving theory exam.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question:\n",
    "    {query}\n",
    "    \n",
    "    Think step-by-step and provide a detailed answer based on the context. If the answer is not in the context, say \"I'm sorry, I don't have enough information to answer that question.\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2088758-3220-4fe3-9f04-d101509cbec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_cot_4o = []\n",
    "for item in ground_truth:\n",
    "    query = item['query']\n",
    "    ground_truth_answer = item['ground_truth_answer']\n",
    "    retrieved_docs, _ = search_function(query)\n",
    "    generated_answer = generate_answer_cot_4o_mini(query, retrieved_docs)\n",
    "    evaluation = evaluate_generated_answer(query, generated_answer, ground_truth_answer)\n",
    "    evaluations_cot_4o.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a5b8197-adeb-4245-a8af-f7ff174eabef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. score: 7.9\n"
     ]
    }
   ],
   "source": [
    "print(f\"Avg. score: {np.mean([json.loads(e)['Score'] for e in evaluations_cot_4o])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ef03e-dbb6-4573-b4dc-27dfa5c56365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c16c3-dfa2-4455-9847-180d023adf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Method 5: Few-Shot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
