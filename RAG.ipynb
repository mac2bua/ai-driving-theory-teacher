{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d0016b8-44f5-48f1-a676-f4b6b7a68fa5",
   "metadata": {},
   "source": [
    "### Prepare Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe0c68fa-fae0-4b40-bc84-479e50aa1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load your generated questions\n",
    "with open('data/generated_questions.json', 'r') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "# Initialize the embedding model\n",
    "model_name = \"multi-qa-distilbert-cos-v1\"\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "# Encode the questions\n",
    "embeddings = []\n",
    "for q in questions:\n",
    "    q_text = f\"{q['theme']} {q['chapter']} {q['question']}\"\n",
    "    embedding = embedding_model.encode(q_text)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ba79e-32d2-4607-a5d6-36cb923b3fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f4ca902-e1cc-42dd-91de-fc4756587849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e2390-ca89-42bb-b39f-1b5d8c9eea8d",
   "metadata": {},
   "source": [
    "### Create and Save FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb8d8ccf-74c1-4a48-9812-d9c049cf1078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Define the dimension\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save the index\n",
    "faiss.write_index(index, 'data/questions_index.faiss')\n",
    "\n",
    "# Save the mapping of index to questions\n",
    "with open('data/index_to_question.json', 'w') as f:\n",
    "    json.dump(questions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6567f-244e-4a64-a599-1c10400333fb",
   "metadata": {},
   "source": [
    "### Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fe426cc-0f46-485b-be27-920d5ad472f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_question(query, index, questions, embedding_model, k=5):\n",
    "    # Encode the query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "\n",
    "    # Search the index\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    # Retrieve the top k questions\n",
    "    retrieved_questions = [questions[idx] for idx in indices[0]]\n",
    "    return retrieved_questions, indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a9263b-a241-4193-8b28-b0f9c0dbf0de",
   "metadata": {},
   "source": [
    "### RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78ecaee4-b5ff-4a68-a40d-32f86d9e17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "def generate_answer(query, retrieved_docs):\n",
    "    context = \"\\n\".join([f\"Q: {doc['question']}\\nA: {doc['correct_options']}\" for doc in retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant helping users prepare for the German driving theory exam.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer the question using the context above, don't include the letter at the beginning of each correct option. If the answer is not in the context, say \"I'm sorry, I don't have enough information to answer that question.\"\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',  # Use 'gpt-4' if available\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "926ef9f0-02e8-49eb-86ac-7107b4d66024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load questions\n",
    "with open('data/index_to_question.json', 'r') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index('data/questions_index.faiss')\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e285599-330c-46bf-99a4-e8eb16b3af89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The maximum allowable blood alcohol concentration for truck drivers in Germany is 0.02%.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What is the maximum allowed blood alcohol concentration for truck drivers in Germany?\"\n",
    "\n",
    "retrieved_docs, _ = search_question(user_query, index, questions, embedding_model, k=5)\n",
    "answer = generate_answer(user_query, retrieved_docs)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd9397f8-7876-4e19-95c0-dd61f0cb7d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: A red triangular sign indicates \"B. Yield\" according to German traffic signs.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What does a red triangular sign indicate?\"\n",
    "\n",
    "retrieved_docs, _ = search_question(user_query, index, questions, embedding_model, k=5)\n",
    "answer = generate_answer(user_query, retrieved_docs)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "76681a61-8107-419a-9f50-9818ad47d48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Ignoring signs of tiredness while driving can lead to decreased reaction times, impaired judgement, and an increased risk of accidents. It is important to prioritize rest and take breaks when feeling tired to ensure safe driving practices.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What can happen if you ignore signs of tiredness?\"\n",
    "\n",
    "retrieved_docs, _ = search_question(user_query, index, questions, embedding_model, k=5)\n",
    "answer = generate_answer(user_query, retrieved_docs)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0178b11c-5af7-4ffe-be4c-8affe155dc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Possible causes for a vehicle to leave the road could include distractions, drowsiness, oversteering, or understeering.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What could cause the vehicle to leave the road?\"\n",
    "\n",
    "retrieved_docs, _ = search_question(user_query, index, questions, embedding_model, k=5)\n",
    "answer = generate_answer(user_query, retrieved_docs)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f0225-4d25-48f8-91de-52c0a3d3f6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "537973f3-cf9e-4553-9d89-4532f69035db",
   "metadata": {},
   "source": [
    "### Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37cbf938-1ac4-4a14-a8dd-81906a11d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = sum(1 for relevance in relevance_total if any(relevance))\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "    for relevance in relevance_total:\n",
    "        for rank, rel in enumerate(relevance):\n",
    "            if rel:\n",
    "                total_score += 1 / (rank + 1)\n",
    "                break\n",
    "    return total_score / len(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c11a6c8d-e83a-47ab-bb45-5114a67b2679",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ground_truth.json', 'r') as f:\n",
    "    ground_truth = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "325bef44-bab9-4330-a26c-22eaf9ebd34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 97.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate: 1.0\n",
      "MRR: 0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def search_function(query):\n",
    "    return search_question(query, index, questions, embedding_model, k=5)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for item in tqdm(ground_truth):\n",
    "        query = item['query']\n",
    "        relevant_doc_id = item['relevant_doc_id']\n",
    "        results, indices = search_function(query)\n",
    "        relevance = [idx == relevant_doc_id for idx in indices]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }\n",
    "\n",
    "results = evaluate(ground_truth, search_function)\n",
    "print(\"Hit Rate:\", results['hit_rate'])\n",
    "print(\"MRR:\", results['mrr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32977419-46e5-4007-9eda-f8b9cc6e1089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7da8b47d-b6fb-414c-8f80-b1279f78f632",
   "metadata": {},
   "source": [
    "## RAG Evaluation (LLM-as-a-jugde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a0b229d-59f9-4fd6-9380-fce1d676850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generated_answer(query, generated_answer, ground_truth_answer):\n",
    "    prompt = \"\"\"\n",
    "    You are to grade the following LLM answer on a scale from 0 to 10.\n",
    "\n",
    "    **Question:**\n",
    "    {query}\n",
    "\n",
    "    **LLM Answer:**\n",
    "    {generated_answer}\n",
    "\n",
    "    **Correct Answer:**\n",
    "    {ground_truth_answer}\n",
    "\n",
    "    **Task:**\n",
    "    - Assign a score from 0 (completely incorrect) to 10 (completely correct).\n",
    "    - Provide a brief justification for the score.\n",
    "\n",
    "    **Response Format:**\n",
    "    Provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "    {{\n",
    "      \"Score\": [0-10],\n",
    "      \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "    }}\n",
    "\"\"\".strip()\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0473570a-fa50-41e6-a909-519d1a2ec5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = []\n",
    "for item in ground_truth[:5]:\n",
    "    query = item['query']\n",
    "    ground_truth_answer = item['ground_truth_answer']\n",
    "    retrieved_docs, _ = search_function(query)\n",
    "    generated_answer = generate_answer(query, retrieved_docs)\n",
    "    evaluation = evaluate_generated_answer(query, generated_answer, ground_truth_answer)\n",
    "    evaluations.append(evaluation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b0f8f817-f3c7-4188-bac4-01e0fd1b8473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\\n  \"Score\": 7,\\n  \"Explanation\": \"The LLM answer provides relevant information and addresses the question, but there are some inaccuracies and missing details. It demonstrates a good understanding of the topic but could be improved with more specific and accurate information.\"\\n}',\n",
       " '{\\n  \"Score\": 5,\\n  \"Explanation\": \"The LLM answer partially addresses the question but lacks depth and accuracy. It provides some relevant information but fails to fully answer the query. There is room for improvement in providing a more comprehensive and accurate response.\"\\n}',\n",
       " '{\\n  \"Score\": 7,\\n  \"Explanation\": \"The LLM answer provides relevant information and addresses the question, but there are some inaccuracies and missing details compared to the correct answer. It demonstrates a good understanding of the topic but could be more precise.\"\\n}',\n",
       " '{\\n  \"Score\": 5,\\n  \"Explanation\": \"The LLM answer partially addresses the question but lacks depth and accuracy. It provides some relevant information but fails to fully answer the query. There is room for improvement in providing a more comprehensive and accurate response.\"\\n}',\n",
       " '{\\n  \"Score\": 5,\\n  \"Explanation\": \"The LLM answer partially addresses the question but lacks depth and accuracy. It provides some relevant information but fails to fully answer the query. There is room for improvement in providing a more comprehensive and accurate response.\"\\n}']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc97c40-2e99-4537-ac53-0141a84281e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7debfc-d596-42b0-8456-c5081698257b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79352cca-1925-4bf4-8bd4-50c80d8a7a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
