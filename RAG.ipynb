{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d0016b8-44f5-48f1-a676-f4b6b7a68fa5",
   "metadata": {},
   "source": [
    "### Prepare Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe0c68fa-fae0-4b40-bc84-479e50aa1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load your generated questions\n",
    "with open('data/generated_questions.json', 'r') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "# Initialize the embedding model\n",
    "model_name = \"multi-qa-distilbert-cos-v1\"\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "# Encode the questions\n",
    "embeddings = []\n",
    "for q in questions:\n",
    "    q_text = f\"{q['theme']} {q['chapter']} {q['question']}\"\n",
    "    embedding = embedding_model.encode(q_text)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ba79e-32d2-4607-a5d6-36cb923b3fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f4ca902-e1cc-42dd-91de-fc4756587849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e2390-ca89-42bb-b39f-1b5d8c9eea8d",
   "metadata": {},
   "source": [
    "### Create and Save FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8d8ccf-74c1-4a48-9812-d9c049cf1078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Define the dimension\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save the index\n",
    "faiss.write_index(index, 'data/questions_index.faiss')\n",
    "\n",
    "# Save the mapping of index to questions\n",
    "with open('data/index_to_question.json', 'w') as f:\n",
    "    json.dump(questions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6567f-244e-4a64-a599-1c10400333fb",
   "metadata": {},
   "source": [
    "### Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe426cc-0f46-485b-be27-920d5ad472f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_question(query, index, questions, embedding_model, k=5):\n",
    "    # Encode the query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "\n",
    "    # Search the index\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    # Retrieve the top k questions\n",
    "    retrieved_questions = [questions[idx] for idx in indices[0]]\n",
    "    return retrieved_questions, indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a9263b-a241-4193-8b28-b0f9c0dbf0de",
   "metadata": {},
   "source": [
    "### RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ecaee4-b5ff-4a68-a40d-32f86d9e17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "def generate_answer(query, retrieved_docs):\n",
    "    context = \"\\n\".join([f\"Q: {doc['question']}\\nA: {doc['correct_options']}\" for doc in retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant helping users prepare for the German driving theory exam.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer the question using the context above, don't include the letter at the beginning of each correct option. If the answer is not in the context, say \"I'm sorry, I don't have enough information to answer that question.\"\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "926ef9f0-02e8-49eb-86ac-7107b4d66024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load questions\n",
    "with open('data/index_to_question.json', 'r') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index('data/questions_index.faiss')\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e285599-330c-46bf-99a4-e8eb16b3af89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The maximum allowed blood alcohol concentration for truck drivers in Germany is 0.0%.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What is the maximum allowed blood alcohol concentration for truck drivers in Germany?\"\n",
    "\n",
    "retrieved_docs, _ = search_question(user_query, index, questions, embedding_model, k=5)\n",
    "answer = generate_answer(user_query, retrieved_docs)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd9397f8-7876-4e19-95c0-dd61f0cb7d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I'm sorry, I don't have enough information to answer that question.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What does a red triangular sign indicate?\"\n",
    "\n",
    "retrieved_docs, _ = search_question(user_query, index, questions, embedding_model, k=5)\n",
    "answer = generate_answer(user_query, retrieved_docs)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76681a61-8107-419a-9f50-9818ad47d48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I'm sorry, I don't have enough information to answer that question.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What can happen if you ignore signs of tiredness?\"\n",
    "\n",
    "retrieved_docs, _ = search_question(user_query, index, questions, embedding_model, k=5)\n",
    "answer = generate_answer(user_query, retrieved_docs)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0178b11c-5af7-4ffe-be4c-8affe155dc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I'm sorry, I don't have enough information to answer that question.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What could cause the vehicle to leave the road?\"\n",
    "\n",
    "retrieved_docs, _ = search_question(user_query, index, questions, embedding_model, k=5)\n",
    "answer = generate_answer(user_query, retrieved_docs)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f0225-4d25-48f8-91de-52c0a3d3f6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "537973f3-cf9e-4553-9d89-4532f69035db",
   "metadata": {},
   "source": [
    "### Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37cbf938-1ac4-4a14-a8dd-81906a11d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = sum(1 for relevance in relevance_total if any(relevance))\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "    for relevance in relevance_total:\n",
    "        for rank, rel in enumerate(relevance):\n",
    "            if rel:\n",
    "                total_score += 1 / (rank + 1)\n",
    "                break\n",
    "    return total_score / len(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c11a6c8d-e83a-47ab-bb45-5114a67b2679",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ground_truth.json', 'r') as f:\n",
    "    ground_truth = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "325bef44-bab9-4330-a26c-22eaf9ebd34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 38.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate: 1.0\n",
      "MRR: 0.8666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def search_function(query):\n",
    "    return search_question(query, index, questions, embedding_model, k=5)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for item in tqdm(ground_truth):\n",
    "        query = item['query']\n",
    "        relevant_doc_id = item['relevant_doc_id']\n",
    "        results, indices = search_function(query)\n",
    "        relevance = [idx == relevant_doc_id for idx in indices]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }\n",
    "\n",
    "results = evaluate(ground_truth, search_function)\n",
    "print(\"Hit Rate:\", results['hit_rate'])\n",
    "print(\"MRR:\", results['mrr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32977419-46e5-4007-9eda-f8b9cc6e1089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7da8b47d-b6fb-414c-8f80-b1279f78f632",
   "metadata": {},
   "source": [
    "## RAG Evaluation (LLM-as-a-jugde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a0b229d-59f9-4fd6-9380-fce1d676850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generated_answer(query, generated_answer, ground_truth_answer):\n",
    "    prompt = f\"\"\"\n",
    "You are to grade the following LLM answer on a scale from 0 to 10.\n",
    "\n",
    "**Question:**\n",
    "{query}\n",
    "\n",
    "**LLM Answer:**\n",
    "{generated_answer}\n",
    "\n",
    "**Correct Answer:**\n",
    "{ground_truth_answer}\n",
    "\n",
    "**Task:**\n",
    "- Assign a score from 0 (completely incorrect) to 10 (completely correct).\n",
    "- Provide a brief justification for the score.\n",
    "\n",
    "**Response Format:**\n",
    "Provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Score\": [0-10],\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0473570a-fa50-41e6-a909-519d1a2ec5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = []\n",
    "for item in ground_truth:\n",
    "    query = item['query']\n",
    "    ground_truth_answer = item['ground_truth_answer']\n",
    "    retrieved_docs, _ = search_function(query)\n",
    "    generated_answer = generate_answer(query, retrieved_docs)\n",
    "    evaluation = evaluate_generated_answer(query, generated_answer, ground_truth_answer)\n",
    "    evaluations.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eecbeb6c-79be-4ae8-993e-d0cbd59afb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\\n  \"Score\": 10,\\n  \"Explanation\": \"The answer is completely correct and provides the accurate information that truck drivers in Germany are not permitted to have any alcohol in their system while operating a vehicle.\"\\n}',\n",
       " '{\\n  \"Score\": 2,\\n  \"Explanation\": \"The answer provided is incorrect as the legal limit for blood alcohol concentration for truck drivers in Germany is 0.3 grams per liter, not 0.02%. The answer shows a lack of accuracy and understanding of the topic.\"\\n}',\n",
       " '{\\n  \"Score\": 8,\\n  \"Explanation\": \"The answer correctly identifies the action of reducing speed before entering a sharp curve while driving a truck. However, it could be improved by mentioning the specific reason for reducing speed, such as maintaining control of the vehicle and preventing rollovers or loss of control.\"\\n}',\n",
       " '{\\n  \"Score\": 8,\\n  \"Explanation\": \"The answer provided the correct legal limit for blood alcohol concentration (BAC) for truck drivers in Germany. However, it could have been improved by mentioning that the general limit for non-commercial drivers is 0.05% for a more comprehensive answer.\"\\n}',\n",
       " '{\\n  \"Score\": 9,\\n  \"Explanation\": \"The answer provided the correct BAC limit for truck drivers in Germany and mentioned that it is strictly enforced for safety reasons. However, it could have been slightly improved by using the term \\'maximum permissible\\' instead of \\'highest allowable\\'.\"\\n}']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0f8f817-f3c7-4188-bac4-01e0fd1b8473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. score: 8.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Avg. score: {np.mean([json.loads(e)['Score'] for e in evaluations])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc97c40-2e99-4537-ac53-0141a84281e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7debfc-d596-42b0-8456-c5081698257b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79352cca-1925-4bf4-8bd4-50c80d8a7a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
